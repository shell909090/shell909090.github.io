<!doctype html>
<html lang="en-us">
  <head>
    <title>查找重复文件 // Shell&#39;s Home</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.152.0-DEV">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shell Xu" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WH8XZZ4WLY"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WH8XZZ4WLY');
        }
      </script>
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="查找重复文件">
  <meta name="twitter:description" content="算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。
$find . -name &#34;*&#34; -type f -size &#43;0 -exec md5sum {} ; | sort | uniq -d -w 32 原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。
问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。
$find . -name &#34;*&#34; -type f -size &#43;0 -exec md5sum {} ; | sort &gt; file_md5 $cat file_md5 | uniq -d -w 32 $grep &#34;...&#34; file_md5 相信大家都看出是怎么回事情了，就不赘言了。
Windows下以前我总是执行不出，原因在于要这么写。
find . -type f -size &#43;0 -exec md5sum {} ; | sort &gt; report.txt 差一个转义，因为不需要。
总结一下，我们可以用一个脚本来处理这些问题。
--------------------------fine_rep------------------------------ find $1 -type f -size &#43;0 -exec md5sum {} ; | sort &gt; &#34;file_md5&#34; cat &#34;file_md5&#34; | uniq -d -w 32 | cut -d&#34; &#34; -f1 | while read x do grep &#34;$x&#34; file_md5 echo &#34;&#34; done echo &#34;done&#34; rm &#34;file_md5&#34; ------------------------------------------------------------">

    <meta property="og:url" content="//blog.shell909090.org/blog/archives/348/">
  <meta property="og:site_name" content="Shell&#39;s Home">
  <meta property="og:title" content="查找重复文件">
  <meta property="og:description" content="算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。
$find . -name &#34;*&#34; -type f -size &#43;0 -exec md5sum {} ; | sort | uniq -d -w 32 原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。
问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。
$find . -name &#34;*&#34; -type f -size &#43;0 -exec md5sum {} ; | sort &gt; file_md5 $cat file_md5 | uniq -d -w 32 $grep &#34;...&#34; file_md5 相信大家都看出是怎么回事情了，就不赘言了。
Windows下以前我总是执行不出，原因在于要这么写。
find . -type f -size &#43;0 -exec md5sum {} ; | sort &gt; report.txt 差一个转义，因为不需要。
总结一下，我们可以用一个脚本来处理这些问题。
--------------------------fine_rep------------------------------ find $1 -type f -size &#43;0 -exec md5sum {} ; | sort &gt; &#34;file_md5&#34; cat &#34;file_md5&#34; | uniq -d -w 32 | cut -d&#34; &#34; -f1 | while read x do grep &#34;$x&#34; file_md5 echo &#34;&#34; done echo &#34;done&#34; rm &#34;file_md5&#34; ------------------------------------------------------------">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2007-01-15T07:39:52+08:00">
    <meta property="article:modified_time" content="2007-01-15T07:39:52+08:00">
    <meta property="article:tag" content="Bash">
    <meta property="article:tag" content="Linux">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body);
        });
    </script>

<header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow-night-eighties.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="//blog.shell909090.org/">/home/shell&#39;s home</a>
      </li>
      

      

    </ul>
  </nav>
</header>


  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/avatar.jpg" alt="Shell Xu" /></a>
      <span class="app-header-title">Shell&#39;s Home</span>
      <p>贝壳的壳</p>
      <p>Copyright &copy; 2025 Shell Xu - <a href="//blog.shell909090.org/license/">License</a></p>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">查找重复文件</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Jan 15, 2007
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          1 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/bash/">Bash</a>
              <a class="tag" href="/tags/linux/">Linux</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。</p>
<pre><code>$find . -name &quot;*&quot; -type f -size +0 -exec md5sum {} ; | sort | uniq -d -w 32
</code></pre>
<p>原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。</p>
<p>问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。</p>
<pre><code>$find . -name &quot;*&quot; -type f -size +0 -exec md5sum {} ; | sort &gt; file_md5
$cat file_md5 | uniq -d -w 32
$grep &quot;...&quot; file_md5
</code></pre>
<p>相信大家都看出是怎么回事情了，就不赘言了。</p>
<p>Windows下以前我总是执行不出，原因在于要这么写。</p>
<pre><code>find . -type f -size +0 -exec md5sum {} ; | sort &gt; report.txt
</code></pre>
<p>差一个转义，因为不需要。</p>
<p>总结一下，我们可以用一个脚本来处理这些问题。</p>
<pre><code>--------------------------fine_rep------------------------------

find $1 -type f -size +0 -exec md5sum {} ; | sort &gt; &quot;file_md5&quot;
cat &quot;file_md5&quot; | uniq -d -w 32 | cut -d&quot; &quot; -f1 | while read x
do
	grep &quot;$x&quot; file_md5
	echo &quot;&quot;
done
echo &quot;done&quot;
rm &quot;file_md5&quot;

------------------------------------------------------------
</code></pre>

    </div>
    <div class="post-footer">
      <div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'shell909090blog';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </div>
  </article>

    </main>
  </body>
</html>
