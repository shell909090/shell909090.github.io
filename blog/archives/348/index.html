<!doctype html>
<html lang="en-us">
  <head>
    <title>查找重复文件 // Shell&#39;s Home</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.58.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shell Xu" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="//blog.shell909090.org/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-48816091-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="查找重复文件"/>
<meta name="twitter:description" content="算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。
$find . -name &quot;*&quot; -type f -size &#43;0 -exec md5sum {} ; | sort | uniq -d -w 32  原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。
问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。
$find . -name &quot;*&quot; -type f -size &#43;0 -exec md5sum {} ; | sort &gt; file_md5 $cat file_md5 | uniq -d -w 32 $grep &quot;...&quot; file_md5  相信大家都看出是怎么回事情了，就不赘言了。
Windows下以前我总是执行不出，原因在于要这么写。
find . -type f -size &#43;0 -exec md5sum {} ; | sort &gt; report.txt  差一个转义，因为不需要。
总结一下，我们可以用一个脚本来处理这些问题。
--------------------------fine_rep------------------------------ find $1 -type f -size &#43;0 -exec md5sum {} ; | sort &gt; &quot;file_md5&quot; cat &quot;file_md5&quot; | uniq -d -w 32 | cut -d&quot; &quot; -f1 | while read x do grep &quot;$x&quot; file_md5 echo &quot;&quot; done echo &quot;done&quot; rm &quot;file_md5&quot; ------------------------------------------------------------  "/>

    <meta property="og:title" content="查找重复文件" />
<meta property="og:description" content="算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。
$find . -name &quot;*&quot; -type f -size &#43;0 -exec md5sum {} ; | sort | uniq -d -w 32  原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。
问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。
$find . -name &quot;*&quot; -type f -size &#43;0 -exec md5sum {} ; | sort &gt; file_md5 $cat file_md5 | uniq -d -w 32 $grep &quot;...&quot; file_md5  相信大家都看出是怎么回事情了，就不赘言了。
Windows下以前我总是执行不出，原因在于要这么写。
find . -type f -size &#43;0 -exec md5sum {} ; | sort &gt; report.txt  差一个转义，因为不需要。
总结一下，我们可以用一个脚本来处理这些问题。
--------------------------fine_rep------------------------------ find $1 -type f -size &#43;0 -exec md5sum {} ; | sort &gt; &quot;file_md5&quot; cat &quot;file_md5&quot; | uniq -d -w 32 | cut -d&quot; &quot; -f1 | while read x do grep &quot;$x&quot; file_md5 echo &quot;&quot; done echo &quot;done&quot; rm &quot;file_md5&quot; ------------------------------------------------------------  " />
<meta property="og:type" content="article" />
<meta property="og:url" content="//blog.shell909090.org/blog/archives/348/" />
<meta property="article:published_time" content="2007-01-15T07:39:52+08:00" />
<meta property="article:modified_time" content="2007-01-15T07:39:52+08:00" />

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body);
        });
    </script>

<header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow-night-eighties.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="//blog.shell909090.org/">/home/shell&#39;s home</a>
      </li>
      

      

    </ul>
  </nav>
</header>


  </head>
  <body>
    <header class="app-header">
      <a href="//blog.shell909090.org/"><img class="app-header-avatar" src="/avatar.jpg" alt="Shell Xu" /></a>
      <h1>Shell&#39;s Home</h1>
      <p>贝壳的壳</p>
      <div class="app-header-social">
        
      </div>
      <p>Copyright &copy; 2021 Shell Xu - <a href="//blog.shell909090.org/license/">License</a></p>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">查找重复文件</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 15, 2007
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="//blog.shell909090.org/tags/bash/">bash</a><a class="tag" href="//blog.shell909090.org/tags/linux/">linux</a></div></div>
    </header>
    <div class="post-content">
      <p>算是介绍一个奇淫巧技吧。查找重复的文件，这个应该有很多软件都可以做的。不过在Linux里面，利用系统工具，一句语句查找应该就比较少见了。</p>

<pre><code>$find . -name &quot;*&quot; -type f -size +0 -exec md5sum {} ; | sort | uniq -d -w 32
</code></pre>

<p>原理是这样的，先用find查找当前所有文件。我们加上限定类型必须是文件，目录不要。限定大小不为0，空文件不要。然后对所有满足条件的执行md5sum，获得md5和文件的列表。然后排序，再针对md5的部分做唯一限定。就得到了所有md5相同的文件的列表。</p>

<p>问题是，这时候我们得到的还只是一堆重复的文件的md5。我们可以把以上步骤拆开来获得完整的输出。</p>

<pre><code>$find . -name &quot;*&quot; -type f -size +0 -exec md5sum {} ; | sort &gt; file_md5
$cat file_md5 | uniq -d -w 32
$grep &quot;...&quot; file_md5
</code></pre>

<p>相信大家都看出是怎么回事情了，就不赘言了。</p>

<p>Windows下以前我总是执行不出，原因在于要这么写。</p>

<pre><code>find . -type f -size +0 -exec md5sum {} ; | sort &gt; report.txt
</code></pre>

<p>差一个转义，因为不需要。</p>

<p>总结一下，我们可以用一个脚本来处理这些问题。</p>

<pre><code>--------------------------fine_rep------------------------------

find $1 -type f -size +0 -exec md5sum {} ; | sort &gt; &quot;file_md5&quot;
cat &quot;file_md5&quot; | uniq -d -w 32 | cut -d&quot; &quot; -f1 | while read x
do
    grep &quot;$x&quot; file_md5
    echo &quot;&quot;
done
echo &quot;done&quot;
rm &quot;file_md5&quot;

------------------------------------------------------------
</code></pre>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
